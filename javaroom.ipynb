{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#  一、预测分析·商品评论情感预测\n",
    "竞赛地址： [https://www.heywhale.com/home/competition/609cc718ca31cd0017835fdc/content/1](https://www.heywhale.com/home/competition/609cc718ca31cd0017835fdc/content/1)\n",
    "希望大家用PaddleNLP把它打趴下，地下爬。。。。。。\n",
    "## 1.背景\n",
    "众所周知，大数据是企业的基本生产资料，数据信息是企业 宝贵的资产。不同于其他资产，数据资产主要在企业运营过程中 产生，较易获取，但要持续积累、沉淀和做好管理却并不容易， 这是一项长期且系统性的工程。未经“雕琢”的数据是一组无序、 混乱的数字，并不能给企业带来何种价值，从庞杂晦涩的数据中 挖掘出“宝藏”充满着挑战，这需要将业务、技术与管理三者相 互融合起来进行创新。\n",
    "\n",
    "随着网上购物越来越流行，人们对于网上购物的需求变得越来越高，这让京东，淘宝等电商平台得到了很大的发展机遇。但是，这种需求也推动了更多的电商平台的发展，引发了激烈的竞争。在这种电商平台激烈竞争的大背景下，除了提高商品质量，压低商品价格外，了解更多的消费者心声对于电商平台来说也越来越重要。其中非常重要的一种方式就是针对消费者的购物行为数据和文本评论数据进行内在信息的数据挖掘分析。而得到这些信息，也有利于对应商品的生产自身竞争力的提高，以及为用户提供高质量感兴趣的商品。\n",
    "\n",
    "## 2.数据简介\n",
    "* 本数据集包括52 万件商品，1100 多个类目，142 万用户，720 万条评论/评分数据\n",
    "* 本次练习赛所使用数据集基于JD的电商数据，来自WWW的JD.com E-Commerce Data，并且针对部分字段做出了一定的调整，所有的字段信息请以本练习赛提供的字段信息为准\n",
    "* 评分为[1,5] 之间的整数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 二、数据初步处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install -U paddlenlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.解压数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submission.csv\n",
      "商品信息.csv\n",
      "商品类别列表.csv\n",
      "测试集.csv\n",
      "训练集.csv\n"
     ]
    }
   ],
   "source": [
    "!tar -xvf data/data96333/商品评论情感预测.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据ID,用户ID,商品ID,评论时间戳,评论标题,评论内容,评分\r\n",
      "TRAIN_0,300212.0,PRODUCT_60357,1282579200,刚到!!!!!!!!,\"刚刚收到,2天我晕,一般快递最快到我们这要3天呢,赞个!!!   包装台简单了,说明书看不懂。 瓶子半透明。   问了官方,说卓越也是他们的合作伙伴,正品放心。\",4.0\r\n",
      "TRAIN_1,213838.0,PRODUCT_354315,1305561600,很好的一本书,不过这本书没有赠送什么代金券。体现不出以前的正版图书送网站学习代金券的特点。,5.0\r\n",
      "TRAIN_2,1045492.0,PRODUCT_192005,1357747200,二手手机,\"很负责任的说一句,亚马逊给我发过来的手机绝对是二手的!!\",1.0\r\n",
      "TRAIN_3,587784.0,PRODUCT_1531,1305129600,送的光盘不行,\"这本书内容很好,就是送的光盘不行。这次重新订购了一套,期望发过来的光盘能用\",4.0\r\n",
      "TRAIN_4,1244067.0,PRODUCT_324528,1285689600,很实用,\"很实用的一本书,非常喜欢!\",5.0\r\n",
      "TRAIN_5,3361.0,PRODUCT_4163,1346256000,关于书籍的包装,\"书籍本身没有问题,货物的包装实在不敢恭维。不知出于何种考虑,先前的纸盒包装现在换成了塑料袋,拍下的两本精装书拿到手居然卷了边,超级郁闷。以此种方式来降低成本,实在不足取。省下的只是仨瓜俩枣,失去的却是人们的信任。\",4.0\r\n",
      "TRAIN_6,83841.0,PRODUCT_114046,1341849600,挺好的,\"包装很好,内容也不错\",4.0\r\n",
      "TRAIN_7,174475.0,PRODUCT_100236,1226505600,便宜点最好了,希望能尽快便宜一些!,4.0\r\n",
      "TRAIN_8,395880.0,PRODUCT_184161,1340812800,物流 包装 一如既往,对于自主游玩川渝还是很有帮助的,5.0\r\n"
     ]
    }
   ],
   "source": [
    "!head 训练集.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据ID,用户ID,商品ID,评论时间戳,评论标题,评论内容\r\n",
      "TEST_0,1013654.0,PRODUCT_176056,1382025600,东西不错,\"大三元之一 东西看上去不错,包装也都很好,关键是价格比京东便宜很多。 还没试过,回去试一下。 不足是不能开增票。比较遗憾\"\r\n",
      "TEST_1,99935.0,PRODUCT_130680,1296144000,这么丰富的经历没写出来,\"这么丰富的经历没写出来,对于我们以后上哪玩挺有帮助,作为游记一般吧。\"\r\n",
      "TEST_2,307768.0,PRODUCT_323370,1303142400,很喜欢 支持离歌 支持饶雪漫~~,很喜欢 支持离歌 支持饶雪漫~~\r\n",
      "TEST_3,152011.0,PRODUCT_383545,1313510400,\"内容空洞,不值得买\",\"内容很空洞,有炫富意味,其它的倒还真没看出什么所以然来。很后悔买了这本书。完全想废纸一样。\"\r\n",
      "TEST_4,1070630.0,PRODUCT_346185,1272556800,爱自己多一点,\"这个书的内容总的来说不错的,书名有点夸张,但看了内容后,发现真的很实实在在的,一点也不夸大。本人特别喜欢后面部分关于鼓舞的内容。一个女人天生长得美人见人爱,而长得不好看的有很多人都自卑,于是总想方设法运用各种化妆品来装饰自己,以此来让别人喜欢自己。看了这个书的内容,很感动,并不是说她的观点如何的好,而是这样的观点出在减肥书上,不漂亮没关系,对自己自信一点,对周围的人更关心一点,你也可以由内而外变得越来越美丽,每天给自己一个小小的肯定,对自己说OK。\"\r\n",
      "TEST_5,1133263.0,PRODUCT_247806,1336060800,\"易懂,好用\",程博士写的书易懂好用!\r\n",
      "TEST_6,42055.0,PRODUCT_82381,1324742400,火机油,\"收到时外包装没问题,但奇怪的是里面瓶身上角有些挤变形了,还好没破,没有泄漏。除去包装外,满意。\"\r\n",
      "TEST_7,1433.0,PRODUCT_457115,1338134400,不错的书,\"不错的书,价格合适,质量还行\"\r\n",
      "TEST_8,650346.0,PRODUCT_348453,1337097600,翻译它最大,\"很喜欢里面的翻译讲解,用四步定位来解决每一个翻译题,屡试屡爽!\"\r\n"
     ]
    }
   ],
   "source": [
    "!head 测试集.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据ID,评分\r\n",
      "TEST_0,1\r\n",
      "TEST_1,1\r\n",
      "TEST_2,1\r\n",
      "TEST_3,1\r\n",
      "TEST_4,1\r\n",
      "TEST_5,1\r\n",
      "TEST_6,1\r\n",
      "TEST_7,1\r\n",
      "TEST_8,1\r\n"
     ]
    }
   ],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.重写read方法读取自定义数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\r\n",
    "from paddle.io import Dataset, Subset\r\n",
    "from paddlenlp.datasets import MapDataset\r\n",
    "import re\r\n",
    "\r\n",
    "\r\n",
    "# 数据ID,用户ID,商品ID,评论时间戳,评论标题,评论内容,评分\r\n",
    "def read(data_path):\r\n",
    "    with open(data_path, 'r', encoding='utf-8') as in_f:\r\n",
    "        next(in_f)\r\n",
    "        for line in in_f:\r\n",
    "            line = line.strip('\\n')\r\n",
    "            split_array = [i.start() for i in re.finditer(',', line)]\r\n",
    "            id = line[:split_array[0]]\r\n",
    "            comment_title = line[split_array[3] + 1:split_array[4]]\r\n",
    "            comment = line[split_array[4] + 2:split_array[-2]]\r\n",
    "            label = line[split_array[-1] + 1:]\r\n",
    "            yield {'text': comment_title  +' '+ comment, 'label': str(int(label.split('.')[0])-1), 'qid': id}\r\n",
    "\r\n",
    "# 数据ID,用户ID,商品ID,评论时间戳,评论标题,评论内容,评分\r\n",
    "def read_test(data_path):\r\n",
    "    with open(data_path, 'r', encoding='utf-8') as in_f:\r\n",
    "        next(in_f)\r\n",
    "        for line in in_f:\r\n",
    "            line = line.strip('\\n')\r\n",
    "            split_array = [i.start() for i in re.finditer(',', line)]\r\n",
    "            id = line[:split_array[0]]\r\n",
    "            id=id.split('_')[-1]\r\n",
    "            comment_title = line[split_array[3] + 1:split_array[4]]\r\n",
    "            comment = line[split_array[4] + 2:split_array[-2]]\r\n",
    "            label= '1'\r\n",
    "            yield {'text': comment_title  +' '+ comment, 'label': label, 'qid': id}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.训练集载入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_path为read()方法的参数\r\n",
    "dataset_ds = load_dataset(read, data_path='训练集.csv',lazy=False)\r\n",
    "# 在这进行划分\r\n",
    "train_ds = Subset(dataset=dataset_ds, indices=[i for i in range(len(dataset_ds)) if i % 10 != 1])\r\n",
    "dev_ds = Subset(dataset=dataset_ds, indices=[i for i in range(len(dataset_ds)) if i % 10 == 1])\r\n",
    "\r\n",
    "test_ds =  load_dataset(read_test, data_path='测试集.csv',lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '东西不错 大三元之一 东西看上去不错,包装也都很好', 'label': '1', 'qid': '0'}\n",
      "{'text': '这么丰富的经历没写出来 这么丰富的经历没写出来', 'label': '1', 'qid': '1'}\n",
      "{'text': '很喜欢 支持离歌 支持饶雪漫~~ ', 'label': '1', 'qid': '2'}\n",
      "{'text': '\"内容空洞 值得买\",\"内容很空洞', 'label': '1', 'qid': '3'}\n",
      "{'text': '爱自己多一点 这个书的内容总的来说不错的,书名有点夸张,但看了内容后,发现真的很实实在在的,一点也不夸大。本人特别喜欢后面部分关于鼓舞的内容。一个女人天生长得美人见人爱,而长得不好看的有很多人都自卑,于是总想方设法运用各种化妆品来装饰自己,以此来让别人喜欢自己。看了这个书的内容,很感动,并不是说她的观点如何的好,而是这样的观点出在减肥书上,不漂亮没关系,对自己自信一点,对周围的人更关心一点,你也可以由内而外变得越来越美丽', 'label': '1', 'qid': '4'}\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\r\n",
    "    print(test_ds[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63000\n",
      "7000\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "# 在转换为MapDataset类型\r\n",
    "train_ds = MapDataset(train_ds)\r\n",
    "dev_ds = MapDataset(dev_ds)\r\n",
    "test_ds = MapDataset(test_ds)\r\n",
    "print(len(train_ds))\r\n",
    "print(len(dev_ds))\r\n",
    "print(len(test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#  三、SKEP模型加载\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/1a4b76447dae404caa3bf123ea28e375179cb09a02de4bef8a2f172edc6e3c8f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2021-06-23 00:15:45,636] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.pdparams\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.weight. classifier.weight is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "/opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1297: UserWarning: Skip loading for classifier.bias. classifier.bias is not found in the provided dict.\n",
      "  warnings.warn((\"Skip loading for {}. \".format(key) + str(err)))\n",
      "[2021-06-23 00:15:56,142] [    INFO] - Found /home/aistudio/.paddlenlp/models/skep_ernie_1.0_large_ch/skep_ernie_1.0_large_ch.vocab.txt\n"
     ]
    }
   ],
   "source": [
    "# 指定模型名称一键加载模型\r\n",
    "from paddlenlp.transformers import SkepForSequenceClassification, SkepTokenizer\r\n",
    "\r\n",
    "model = SkepForSequenceClassification.from_pretrained(\r\n",
    "    'skep_ernie_1.0_large_ch', num_classes=  5)\r\n",
    "# 指定模型名称一键加载tokenizer\r\n",
    "tokenizer = SkepTokenizer.from_pretrained('skep_ernie_1.0_large_ch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 四、数据NLP特征处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\r\n",
    "from functools import partial\r\n",
    "\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import paddle\r\n",
    "import paddle.nn.functional as F\r\n",
    "from paddlenlp.data import Stack, Tuple, Pad\r\n",
    "\r\n",
    "from utils import create_dataloader\r\n",
    "\r\n",
    "def convert_example(example,\r\n",
    "                    tokenizer,\r\n",
    "                    max_seq_length=512,\r\n",
    "                    is_test=False):\r\n",
    "   \r\n",
    "    # 将原数据处理成model可读入的格式，enocded_inputs是一个dict，包含input_ids、token_type_ids等字段\r\n",
    "    encoded_inputs = tokenizer(\r\n",
    "        text=example[\"text\"], max_seq_len=max_seq_length)\r\n",
    "\r\n",
    "    # input_ids：对文本切分token后，在词汇表中对应的token id\r\n",
    "    input_ids = encoded_inputs[\"input_ids\"]\r\n",
    "    # token_type_ids：当前token属于句子1还是句子2，即上述图中表达的segment ids\r\n",
    "    token_type_ids = encoded_inputs[\"token_type_ids\"]\r\n",
    "\r\n",
    "    if not is_test:\r\n",
    "        # label：情感极性类别\r\n",
    "        label = np.array([example[\"label\"]], dtype=\"int64\")\r\n",
    "        return input_ids, token_type_ids, label\r\n",
    "    else:\r\n",
    "        # qid：每条数据的编号\r\n",
    "        qid = np.array([example[\"qid\"]], dtype=\"int64\")\r\n",
    "        return input_ids, token_type_ids, qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import create_dataloader\r\n",
    "# 处理的最大文本序列长度\r\n",
    "max_seq_length=256\r\n",
    "# 批量数据大小\r\n",
    "batch_size=35\r\n",
    "\r\n",
    "# 将数据处理成模型可读入的数据格式\r\n",
    "trans_func = partial(\r\n",
    "    convert_example,\r\n",
    "    tokenizer=tokenizer,\r\n",
    "    max_seq_length=max_seq_length)\r\n",
    "\r\n",
    "# 将数据组成批量式数据，如\r\n",
    "# 将不同长度的文本序列padding到批量式数据中最大长度\r\n",
    "# 将每条数据label堆叠在一起\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input_ids\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # token_type_ids\r\n",
    "    Stack()  # labels\r\n",
    "): [data for data in fn(samples)]\r\n",
    "train_data_loader = create_dataloader(\r\n",
    "    train_ds,\r\n",
    "    mode='train',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)\r\n",
    "dev_data_loader = create_dataloader(\r\n",
    "    dev_ds,\r\n",
    "    mode='dev',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 五、模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.训练准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\r\n",
    "\r\n",
    "from utils import evaluate\r\n",
    "\r\n",
    "# 训练轮次\r\n",
    "epochs = 10\r\n",
    "# 训练过程中保存模型参数的文件夹\r\n",
    "ckpt_dir = \"skep_ckpt\"\r\n",
    "# len(train_data_loader)一轮训练所需要的step数\r\n",
    "num_training_steps = len(train_data_loader) * epochs\r\n",
    "\r\n",
    "# Adam优化器\r\n",
    "optimizer = paddle.optimizer.AdamW(\r\n",
    "    learning_rate=2e-5,\r\n",
    "    parameters=model.parameters())\r\n",
    "# 交叉熵损失函数\r\n",
    "criterion = paddle.nn.loss.CrossEntropyLoss()\r\n",
    "# accuracy评价指标\r\n",
    "metric = paddle.metric.Accuracy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 开启训练\r\n",
    "\r\n",
    "# 加入日志显示\r\n",
    "from visualdl import LogWriter\r\n",
    "\r\n",
    "writer = LogWriter(\"./log\")\r\n",
    "best_val_acc=0\r\n",
    "global_step = 0\r\n",
    "tic_train = time.time()\r\n",
    "for epoch in range(1, epochs + 1):\r\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\r\n",
    "        input_ids, token_type_ids, labels = batch\r\n",
    "        # 喂数据给model\r\n",
    "        logits = model(input_ids, token_type_ids)\r\n",
    "        # 计算损失函数值\r\n",
    "        loss = criterion(logits, labels)\r\n",
    "        # 预测分类概率值\r\n",
    "        probs = F.softmax(logits, axis=1)\r\n",
    "        # 计算acc\r\n",
    "        correct = metric.compute(probs, labels)\r\n",
    "        metric.update(correct)\r\n",
    "        acc = metric.accumulate()\r\n",
    "\r\n",
    "        global_step += 1\r\n",
    "        if global_step % 10 == 0:\r\n",
    "            print(\r\n",
    "                \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\r\n",
    "                % (global_step, epoch, step, loss, acc,\r\n",
    "                    10 / (time.time() - tic_train)))\r\n",
    "            tic_train = time.time()\r\n",
    "        \r\n",
    "        # 反向梯度回传，更新参数\r\n",
    "        loss.backward()\r\n",
    "        optimizer.step()\r\n",
    "        optimizer.clear_grad()\r\n",
    "\r\n",
    "        if global_step % 100 == 0:\r\n",
    "            # 评估当前训练的模型\r\n",
    "            eval_loss, eval_accu = evaluate(model, criterion, metric, dev_data_loader)\r\n",
    "            print(\"eval  on dev  loss: {:.8}, accu: {:.8}\".format(eval_loss, eval_accu))\r\n",
    "            # 加入eval日志显示\r\n",
    "            writer.add_scalar(tag=\"eval/loss\", step=global_step, value=eval_loss)\r\n",
    "            writer.add_scalar(tag=\"eval/acc\", step=global_step, value=eval_accu)\r\n",
    "            # 加入train日志显示\r\n",
    "            writer.add_scalar(tag=\"train/loss\", step=global_step, value=loss)\r\n",
    "            writer.add_scalar(tag=\"train/acc\", step=global_step, value=acc)\r\n",
    "            save_dir = \"best_checkpoint\"\r\n",
    "            # 加入保存       \r\n",
    "            if eval_accu>best_val_acc:\r\n",
    "                if not os.path.exists(save_dir):\r\n",
    "                    os.mkdir(save_dir)\r\n",
    "                best_val_acc=eval_accu\r\n",
    "                print(f\"模型保存在 {global_step} 步， 最佳eval准确度为{best_val_acc:.8f}！\")\r\n",
    "                save_param_path = os.path.join(save_dir, 'best_model.pdparams')\r\n",
    "                paddle.save(model.state_dict(), save_param_path)\r\n",
    "                fh = open('best_checkpoint/best_model.txt', 'w', encoding='utf-8')\r\n",
    "                fh.write(f\"模型保存在 {global_step} 步， 最佳eval准确度为{best_val_acc:.8f}！\")\r\n",
    "                fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.训练日志\n",
    "visual dl 就不放了\n",
    "\n",
    "```\n",
    "模型保存在 2000 步， 最佳eval准确度为0.66757143！\n",
    "global step 2010, epoch: 2, batch: 210, loss: 0.49394, accu: 0.66286, speed: 0.09 step/s\n",
    "global step 2020, epoch: 2, batch: 220, loss: 0.66231, accu: 0.65429, speed: 0.90 step/s\n",
    "global step 2030, epoch: 2, batch: 230, loss: 0.67677, accu: 0.66286, speed: 0.86 step/s\n",
    "global step 2040, epoch: 2, batch: 240, loss: 0.75220, accu: 0.67143, speed: 0.90 step/s\n",
    "global step 2050, epoch: 2, batch: 250, loss: 0.66303, accu: 0.67600, speed: 1.17 step/s\n",
    "global step 2060, epoch: 2, batch: 260, loss: 0.67201, accu: 0.67857, speed: 0.79 step/s\n",
    "global step 2070, epoch: 2, batch: 270, loss: 1.00059, accu: 0.67224, speed: 0.76 step/s\n",
    "global step 2080, epoch: 2, batch: 280, loss: 0.74657, accu: 0.67786, speed: 0.93 step/s\n",
    "global step 2090, epoch: 2, batch: 290, loss: 0.70754, accu: 0.67778, speed: 0.92 step/s\n",
    "global step 2100, epoch: 2, batch: 300, loss: 0.74980, accu: 0.68257, speed: 0.84 step/s\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 六、预测提交结果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1.测试数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "test_ds =  load_dataset(read_test, data_path='测试集.csv',lazy=False)\r\n",
    "# 在转换为MapDataset类型\r\n",
    "test_ds = MapDataset(test_ds)\r\n",
    "print(len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\r\n",
    "import paddle\r\n",
    "\r\n",
    "# 处理测试集数据\r\n",
    "trans_func = partial(\r\n",
    "    convert_example,\r\n",
    "    tokenizer=tokenizer,\r\n",
    "    max_seq_length=max_seq_length,\r\n",
    "    is_test=True)\r\n",
    "batchify_fn = lambda samples, fn=Tuple(\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input\r\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),  # segment\r\n",
    "    Stack() # qid\r\n",
    "): [data for data in fn(samples)]\r\n",
    "test_data_loader = create_dataloader(\r\n",
    "    test_ds,\r\n",
    "    mode='test',\r\n",
    "    batch_size=batch_size,\r\n",
    "    batchify_fn=batchify_fn,\r\n",
    "    trans_fn=trans_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2.加载预测模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded parameters from best_checkpoint/best_model.pdparams\n"
     ]
    }
   ],
   "source": [
    "# 根据实际运行情况，更换加载的参数路径\r\n",
    "params_path = 'best_checkpoint/best_model.pdparams'\r\n",
    "if params_path and os.path.isfile(params_path):\r\n",
    "    # 加载模型参数\r\n",
    "    state_dict = paddle.load(params_path)\r\n",
    "    model.set_dict(state_dict)\r\n",
    "    print(\"Loaded parameters from %s\" % params_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.开始预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 处理测试集数据\r\n",
    "label_map = {0: '1', 1:'2', 2:'3', 3:'4',4:'5'}\r\n",
    "results = []\r\n",
    "# 切换model模型为评估模式，关闭dropout等随机因素\r\n",
    "model.eval()\r\n",
    "for batch in test_data_loader:\r\n",
    "    input_ids, token_type_ids, qids = batch\r\n",
    "    # 喂数据给模型\r\n",
    "    logits = model(input_ids, token_type_ids)\r\n",
    "    # 预测分类\r\n",
    "    probs = F.softmax(logits, axis=-1)\r\n",
    "    idx = paddle.argmax(probs, axis=1).numpy()\r\n",
    "    idx = idx.tolist()\r\n",
    "    labels = [label_map[i] for i in idx]\r\n",
    "    qids = qids.numpy().tolist()\r\n",
    "    results.extend(zip(qids, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4.保存结果\n",
    "根据官网要求写入文件（注意：此处数据集给的submission.csv并不对，严格按照官网来）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 写入预测结果\r\n",
    "with open( \"submission.csv\", 'w', encoding=\"utf-8\") as f:\r\n",
    "    # f.write(\"数据ID,评分\\n\")\r\n",
    "    f.write(\"id,score\\n\")\r\n",
    "\r\n",
    "    for (idx, label) in results:\r\n",
    "        f.write('TEST_'+str(idx[0])+\",\"+label+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 5.检查结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_29990,859948.0,PRODUCT_396203,1378569600,超喜欢,\"包装很结实,正品。不过快递差点劲,周六没人,没事先沟通,扔单位收发室门口就走了,完事才给打电话,去了一看,孤零零一个包裹放在大门紧闭的收发室门外地上,看在东西还全的份上就不计较了。\"\r\n",
      "TEST_29991,10932.0,PRODUCT_493311,1345046400,不错,\"能够从中学习,举一反三就要靠自己啦\"\r\n",
      "TEST_29992,58752.0,PRODUCT_8622,1331308800,还好,还没做  大致看看还可以  希望对我有帮助\r\n",
      "TEST_29993,627968.0,PRODUCT_67330,1350144000,真的不错,\"在商场里看完新秀丽,再回家等待亚马逊送货。打开后一看跟新秀丽差不多,十分满意。除了拉杆没试过,其他配件质量不错,且灰色耐脏,此次购物较为满意。\"\r\n",
      "TEST_29994,25132.0,PRODUCT_211610,1214841600,价格变化太快,\"第一次9.8元时没订上,后来涨到13.5,等降到11.2赶紧买了一本,给朋友订时,又降到了11元,没几天,又降价了:10.6元。(真是防不胜防啊——范伟!)这个版本有点瑕疵:前言第一页最后一行“的”字错为“在”。“斯多亚”通译“斯多葛”,其他何译本已经校改,独三联照旧。除此,就印装质量、版式设计来说,还是比较喜欢三联版,希望三联提高质量、不负众望。\"\r\n",
      "TEST_29995,96874.0,PRODUCT_499694,1362844800,不错的教辅书,\"内容挺不错的,讲的贴近考试内容\"\r\n",
      "TEST_29996,39298.0,PRODUCT_27125,1367251200,好,\"东西很不错,用了以后感觉很好\"\r\n",
      "TEST_29997,85102.0,PRODUCT_79396,1326124800,性价比高,\"这款润唇膏用了第二支,滋润度不错,淡淡的柠檬味很清新,价格低。\"\r\n",
      "TEST_29998,405625.0,PRODUCT_441848,1350403200,与我要求商品尺寸不符合,\"请仔细查看我订单中对于商品尺寸的要求,贵商品的尺寸正好符合apple ipad的尺寸要求而不符合我对商品尺寸的要求,请下次发送商品时注意订单中要求,下不为例。\"\r\n",
      "TEST_29999,48032.0,PRODUCT_202241,1353686400,刚收到还没有用,\"刚收到,还没有用不知道怎么样\"\r\n"
     ]
    }
   ],
   "source": [
    "!tail 测试集.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id,score\r\n",
      "TEST_0,5\r\n",
      "TEST_1,5\r\n",
      "TEST_2,5\r\n",
      "TEST_3,2\r\n",
      "TEST_4,5\r\n",
      "TEST_5,4\r\n",
      "TEST_6,4\r\n",
      "TEST_7,5\r\n",
      "TEST_8,5\r\n"
     ]
    }
   ],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST_29990,5\r\n",
      "TEST_29991,5\r\n",
      "TEST_29992,4\r\n",
      "TEST_29993,5\r\n",
      "TEST_29994,4\r\n",
      "TEST_29995,5\r\n",
      "TEST_29996,5\r\n",
      "TEST_29997,5\r\n",
      "TEST_29998,3\r\n",
      "TEST_29999,3\r\n"
     ]
    }
   ],
   "source": [
    "!tail submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 6.提交\n",
    "\n",
    "利用所学知识，取得第六的成绩，如下图：\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/cf5dc3fca8bc46f9af2ad960a5d68fbd5aca7bc93fc3400494b4f85c054520d0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 2.1.0 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
